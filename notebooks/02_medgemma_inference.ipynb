{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-15T19:59:00.290792Z",
     "iopub.status.busy": "2026-02-15T19:59:00.290538Z",
     "iopub.status.idle": "2026-02-15T19:59:04.683320Z",
     "shell.execute_reply": "2026-02-15T19:59:04.682369Z",
     "shell.execute_reply.started": "2026-02-15T19:59:00.290763Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All packages installed!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SENTINELA FLUVIAL - MEDGEMMA 4B INFERENCE PIPELINE\n",
    "# ==============================================================================\n",
    "# Installing dependencies for 4-bit execution (memory efficient)\n",
    "!pip install -q transformers\n",
    "\n",
    "print(\"✅ All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:59:04.685313Z",
     "iopub.status.busy": "2026-02-15T19:59:04.685063Z",
     "iopub.status.idle": "2026-02-15T19:59:05.378203Z",
     "shell.execute_reply": "2026-02-15T19:59:05.377380Z",
     "shell.execute_reply.started": "2026-02-15T19:59:04.685284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# HF token from Kaggle secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)\n",
    "\n",
    "print(\"✅ Logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "print(f\"Loading model and tokenizer: {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset and Output paths\n",
    "dataset_path = '/kaggle/input/datasets/richardsonsouza/dataset-prompts-v2/dataset_prompts.jsonl'\n",
    "# Note: /kaggle/input is read-only. We save to /kaggle/working/ and then move if needed.\n",
    "output_path = '/kaggle/working/result.json'\n",
    "\n",
    "print(f\"STARTING BATCH PROCESSING: {dataset_path}\\n\")\n",
    "\n",
    "# Generation Configuration\n",
    "gen_config = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0.2,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.95\n",
    "}\n",
    "\n",
    "# Test limit (Set to None to process the entire file)\n",
    "LIMITE_TESTE = 6\n",
    "contador = 0\n",
    "results = []\n",
    "\n",
    "try:\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # 1. Safety stop for testing\n",
    "            if LIMITE_TESTE and contador >= LIMITE_TESTE:\n",
    "                print(f\"Test limit reached ({LIMITE_TESTE} cases). Stopping...\")\n",
    "                break\n",
    "                \n",
    "            # 2. JSON Parsing\n",
    "            case = json.loads(line)\n",
    "            \n",
    "            # 3. Metadata Extraction\n",
    "            meta = case.get('meta', {})\n",
    "            municipio = meta.get('municipio', 'Unknown')\n",
    "            competencia = meta.get('competencia', 'N/A')\n",
    "            estacao = meta.get('estacao', 'N/A')\n",
    "            \n",
    "            # 4. Prompt Extraction\n",
    "            prompt_text = case.get('prompt')\n",
    "            \n",
    "            if not prompt_text:\n",
    "                print(f\"Skip: Empty prompt for ID {case.get('id')}\")\n",
    "                continue\n",
    "\n",
    "            # 5. Model Formatting\n",
    "            chat = [\n",
    "                { \"role\": \"user\", \"content\": prompt_text },\n",
    "            ]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            \n",
    "            # 6. Inference\n",
    "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, **gen_config)\n",
    "            \n",
    "            # 7. Decoding (Extracting only the new response)\n",
    "            generated_tokens = outputs[0][inputs.input_ids.shape[-1]:]\n",
    "            clean_response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "            # 8. Report Display\n",
    "            print(f\"CASE {contador + 1}: {municipio.upper()} ({competencia}) | Season: {estacao}\")\n",
    "            print(\"-\" * 80)\n",
    "            print(\"MEDGEMMA RESPONSE (JSON):\")\n",
    "            print(clean_response)\n",
    "            print(\"=\" * 80 + \"\\n\")\n",
    "            \n",
    "            # 9. Result Persistence and Formatting\n",
    "            try:\n",
    "                json_response = json.loads(clean_response)\n",
    "                results.append({\n",
    "                    \"id\": case.get('id'),\n",
    "                    \"meta\": meta,\n",
    "                    \"response\": json_response\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Warning: Response is not valid JSON. Storing as raw string.\")\n",
    "                results.append({\n",
    "                    \"id\": case.get('id'),\n",
    "                    \"meta\": meta,\n",
    "                    \"response_raw\": clean_response\n",
    "                })\n",
    "\n",
    "            # 10. Memory Management and Increment\n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            contador += 1\n",
    "\n",
    "    # Saving consolidated results\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "        json.dump(results, out_f, indent=4, ensure_ascii=False)\n",
    "    print(f\"✅ Successfully persisted {len(results)} results to {output_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file not found. Check the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14898831,
     "sourceId": 118534,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 15702965,
     "datasetId": 9493785,
     "sourceId": 14843610,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 15711735,
     "datasetId": 9499328,
     "sourceId": 14851414,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
